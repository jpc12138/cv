## 项目报告

基于改进特征点检测的全景图像拼接方法

## 成员列表

江鹏程 梅佳俊

## 绪论

我们的项目以一篇文章为基础，文章名为“基于改进特征点检测的全景图像拼接方法”。文章的问题是如何改进全景图拼接算法，以提高准确性和效率。该问题的动机是传统的基于SIFT特征检测的方法在图像拼接过程中存在一些问题，例如拼接缝隙、色彩误差和畸变等，因此需要一种更高效、准确的全景图拼接算法来解决这些问题。

在全景图拼接算法领域，传统的基于尺度不变特征变换（SIFT）的方法被广泛应用。SIFT算法具有较高的准确性和鲁棒性，但其计算复杂度较高，且对噪声和畸变较为敏感。为了改进全景图拼接算法，研究者们提出了一些方法。其中之一是 SURF 算法，它通过加速高斯金字塔和积分图像来提高特征点提取的速度。SURF 算法在保持较高准确性的同时，大大提升了运行效率。在这篇文章中使用的特征点提取算法是改进的 Harris 角点检测法。 传统的 Harris 算法用于提取角点的阈值是固定的，故存在忽略有效像素点的可能性。因此研究者提出自适应阈值设置方法，提高检测算法的精确性，避免伪角点的产生。

另一个重要的改进是随机抽样一致性（RANSAC）算法。RANSAC 算法能够从匹配点集中去除误匹配点，从而提高全景图拼接的精度。传统的RANSAC算法根据点的数量进行采样，但对于较大的数据集，计算时间会显著增加。因此，在这篇文章中改进的加权 RANSAC 算法被提出，通过引入权重来加速采样过程，进一步提高了运行效率。

此外，针对全景图拼接中的畸变问题，研究者们提出了各种校正方法。在这篇文章中使用的方法是空间变化法，它通过计算图像之间的几何变换关系，将拼接后的局部畸变进行校正，从而得到更好的拼接效果。

这些改进方法在一定程度上解决了传统全景图拼接方法存在的问题，提高了拼接效果和算法的速度。然而，仍有待进一步研究和改进，尤其是对于特定的畸变情况和复杂场景的处理。

## 方法的具体细节

### 1 改进特征点检测

传统 Harris 角点检测算法在提取角点质量与速度上都有一些局限性，文中对Harris 角点检测算法进行了如下改进：

（1）计算图像在 x、y方向上的梯度 $I_x$、$I_y$，分别求出x、y 方向上的梯度乘积，$I_x^2=I_x\cdot I_x\quad I_y^2=I_y\cdot I_y\quad I_{xy}=I_x\cdot I_y$，并对$I_x^2$、$I_y^2$和$I_{xy}$进行高斯加权。在此过程中我们去除梯度较小的点，只对梯度幅度比较大的像素点进行计算提取，减少了伪关键点的干扰。

（2）由于Harris算法用于提取角点的阈值是固定的，故存在忽略有效像素点的可能性。为此提出自适应阈值设置方法，提高检测算法的精确性，避免伪角点的产生。

  根据下面的公式（1）计算出每个像素的响应值 *R*：
$$
R = det\left(\sum _{x,y}{w(x,y)}\begin{bmatrix} I_x^2 & I_xI_y \\ I_xI_y & I_y^2 \\ \end{bmatrix}\right)-k\left(trace(\sum_{x,y}{w(x,y)}\begin{bmatrix} I_x^2 & I_xI_y \\ I_xI_y & I_y^2 \\ \end{bmatrix})\right)(1)
$$
  其中*w*为窗口函数，代表了窗口下每个像素点的权重，一般采用高斯函数，表示越靠近所选点，其权值越大。$I_x$和$I_y$分别为x和y坐标下的像素位置。k一般取0.04-0.06。

  对响应值进行判定的阈值 T 求解如下：

  设置迭代终止值 *K* 及迭代开始值 $A_0$,$A_0$ 取 R 值矩阵中最大值 Rmax 与最小值 Rmin 的平均值，即公式（2）：
$$
A_0=(Rmax+Rmin)/2 \qquad (2)
$$
  使用 $A_0$ 对 R 值矩阵中的各个像素点进行分类，其中大于$A_0$的分为 $T_1$区域，小于$A_0$的分为 $T_0$ 区域。计算 $T_0$和 $T_1$ 两个区域中像素的平均值 $\rho_1$ 和 $\rho_2$，并通过公式（3）获取 $\rho$ 值：
$$
\rho=(\rho_1+\rho_2)/2 \qquad (3)
$$
  求出 $\rho$ 与 $A_0$ 的差的绝对值，若小于预先设置的 **K** 即停止迭代，输出阈值 $A_0$。若大于预先设置的 **K**，则将 $A_0$的值等于 $\rho$，并进行重复迭代，直到取得符合条件的阈值即暂停。

```python
#改进的Harris角点检测算法

def harris_corner_detection(image, k=0.05, max_iter=300, epsilon=0.01):
    # 图像灰度化
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # 计算图像在x、y方向上的梯度
    dx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
    dy = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)

    # 计算Ix^2, Iy^2, Ixy并进行高斯加权
    ix_square = cv2.GaussianBlur(dx * dx, (3, 3), 0)
    iy_square = cv2.GaussianBlur(dy * dy, (3, 3), 0)
    ixy = cv2.GaussianBlur(dx * dy, (3, 3), 0)

    # 计算响应值R
    det = ix_square * iy_square - ixy * ixy
    trace = ix_square + iy_square
    response = det - k * (trace ** 2)

    # 自适应阈值设置
    max_response = np.max(response)
    min_response = np.min(response)
    A0 = (max_response + min_response) / 2
    
    for _ in range(max_iter):
        # 分类像素点
        T1 = response > A0
        T0 = response <= A0

        # 计算两个区域的平均值
        rho1 = np.mean(response[T1])
        rho2 = np.mean(response[T0])

        # 计算新的阈值a0
        new_A0 = (rho1 + rho2) / 2

        # 判断是否满足停止迭代条件
        if abs(new_A0 - A0) < epsilon:
            break
        A0 = new_A0

    # 使用阈值筛选角点
    corners = np.zeros_like(image)
    corners[response > A0] = 255
    return corners.astype(np.uint8)
```

提取出角点后可以对角点进行匹配，文章没有说明使用何种匹配方法。在这里我使用 SIFT 算法获取描述子， 使用 BFMatcher 进行特征匹配。代码如下：

```python
def sift_feature_extraction(image, keypoints):
    # SIFT特征提取与描述子计算
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    sift = cv2.SIFT_create()
    _, descriptors = sift.compute(gray, keypoints)
    return descriptors

def sift_matching(descriptors1, descriptors2):
    # SIFT特征匹配
    matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
    matches = matcher.match(descriptors1, descriptors2)
    return matches
```

### 2 误匹配点剔除

对特征点进行匹配后通常会存在较多误匹配对，影响后续拼接效果。文中采用改进的 RANSAC 算法进行误匹配点剔除，得到精确匹配结果。

**RANSAC用于剔除错误匹配的算法流程：**

  1.从匹配的点对中选择8个点，使用归一化8点法估算出基础矩阵F。

  2.计算其余的点对到其对应对极线的距离dp,若dp<d,则为内点，否则为外点，记下符合条件的内点数目num。

  3.迭代k次，或者某次得到内点的数目num占有的比例大于等于95%，则停止。选择num最大的基础矩阵作为最终的结果。

**算法改进代码实现：**

1. 此函数为八点法实现 x1、x2为匹配点集合，8点法传入的 x1,x2 分别为对应的8对匹配点，求解线性方程组的解使用的SVD(奇异值分解)算法，U,S,V分别为计算得到的左奇异值、奇异值、右奇异值。

   ```python
   def compute_fundamental(x1, x2):
       n = x1.shape[1]
       if x2.shape[1] != n:
           raise ValueError("Number of points don't match.")
       # build matrix for equations
       A = np.zeros((n, 9))
       for i in range(n):
           A[i] = [x1[0, i] * x2[0, i], x1[0, i] * x2[1, i], x1[0, i] * x2[2, i],
                   x1[1, i] * x2[0, i], x1[1, i] * x2[1, i], x1[1, i] * x2[2, i],
                   x1[2, i] * x2[0, i], x1[2, i] * x2[1, i], x1[2, i] * x2[2, i]]
       # compute linear least square solution
       U, S, V = np.linalg.svd(A)
       F = V[-1].reshape(3, 3)
       # constrain F
       # make rank 2 by zeroing out last singular value
       U, S, V = np.linalg.svd(F)
       S[2] = 0
       F = np.dot(U, np.dot(np.diag(S), V))
       return F / F[2, 2]
   ```

2. 将匹配点进行归一化处理。

   ```python
   def compute_fundamental_normalized(x1, x2):
       n = x1.shape[1]
       if x2.shape[1] != n:
           raise ValueError("Number of points don't match.")
   
       # normalize image coordinates
       x1 = x1 / x1[2]
       mean_1 = np.mean(x1[:2], axis=1)
       S1 = np.sqrt(2) / np.std(x1[:2])
       T1 = np.array([[S1, 0, -S1 * mean_1[0]], [0, S1, -S1 * mean_1[1]], [0, 0, 1]])
       x1 = np.dot(T1, x1)
   
       x2 = x2 / x2[2]
       mean_2 = np.mean(x2[:2], axis=1)
       S2 = np.sqrt(2) / np.std(x2[:2])
       T2 = np.array([[S2, 0, -S2 * mean_2[0]], [0, S2, -S2 * mean_2[1]], [0, 0, 1]])
       x2 = np.dot(T2, x2)
   
       # compute F with the normalized coordinates
       F = compute_fundamental(x1, x2)
       # print (F)
       # reverse normalization
       F = np.dot(T1.T, np.dot(F, T2))
   
       return F / F[2, 2]
   ```

3. 随机选择八对匹配点用于计算矩阵。

   ```python
   def randSeed(good, num = 8):
       eight_point = random.sample(good, num)
       return eight_point
   ```

4. 计算匹配点对的坐标，注意返回坐标为( x , y ) 形式，所以此处增加了一个维度使得返回的为( x , y , 1 )的形式。

   ```python
   def PointCoordinates(eight_points, keypoints1, keypoints2)
       x1 = []
       x2 = []
       tuple_dim = (1.,)
       for i in eight_points:
           tuple_x1 = keypoints1[i[0].queryIdx].pt + tuple_dim
           tuple_x2 = keypoints2[i[0].trainIdx].pt + tuple_dim
           x1.append(tuple_x1)
           x2.append(tuple_x2)
       return np.array(x1, dtype=float), np.array(x2, dtype=float)
   ```

5. 此处为ransac算法计算匹配点与对极线的距离。

   ```python
   def inlier(F,good, keypoints1,keypoints2,confidence):
       num = 0
       ransac_good = []
       x1, x2 = PointCoordinates(good, keypoints1, keypoints2)
       for i in range(len(x2)):
           line = F.dot(x1[i].T)
           #在对极几何中极线表达式为[A B C],Ax+By+C=0,  方向向量可以表示为[-B,A]
           line_v = np.array([-line[1], line[0]])
           err = h = np.linalg.norm(np.cross(x2[i,:2], line_v)/np.linalg.norm(line_v))
           # err = computeReprojError(x1[i], x2[i], F)
           if abs(err) < confidence:
               ransac_good.append(good[i])
               num += 1
       return num, ransac_good
   ```

6. 此函数为ransac算法实现， 从匹配的点对中选择8个点，使用归一化8点法估算出基础矩阵F 。计算其余的点对到其对应对极线的距离dp ,如果dp ≤ d 则该点为内点，否则为外点。记下符合该条件的内点的个数为num迭代k次，或者某次得到内点的数目num占有的比例大于等于95%，则停止。选择num最大的基础矩阵作为最终的结果。保存最优的结果。

   ```python
   def ransac(good, keypoints1, keypoints2, confidence,iter_num):
       Max_num = 0
       good_F = np.zeros([3,3])
       inlier_points = []
       for i in range(iter_num):
           eight_points = randSeed(good)
           x1,x2 = PointCoordinates(eight_points, keypoints1, keypoints2)
           F = compute_fundamental_normalized(x1.T, x2.T)
           num, ransac_good = inlier(F, good, keypoints1, keypoints2, confidence)
           if num > Max_num:
               Max_num = num
               good_F = F
               inlier_points = ransac_good
       print(Max_num, good_F)
       return Max_num, good_F, inlier_points
   ```

### 3 图像拼接

#### 3.1 图像拼接1

我在这里进行图像拼接的想法是：

1. 根据两张待拼接图像提取出的特征点的匹配情况，计算单应矩阵 H。
2. 根据单应矩阵 H 对 image2 进行透视变换。
3. 根据待拼接图片的大小创建一个足够大的图像，将 image1 和变换后的 image2（即 result ）拼合在一起。

```python
def img_sit(keypoints1, keypoints2, image1, image2, inlier_points):
    pts1 = np.float32([keypoints1[m.queryIdx].pt for m in inlier_points])
    pts2 = np.float32([keypoints2[m.trainIdx].pt for m in inlier_points])
    H, _ = cv2.findHomography(pts1, pts2, cv2.RANSAC, 1.0)

    # 获取image的宽度和高度
    h1, w1 = image1.shape[:2]
    h2, w2 = image2.shape[:2]
    
    # 对image2进行透视变换
    result = cv2.warpPerspective(image2, H, (w2, h2))

    # 创建一个全黑的图像作为拼接结果
    result_image = np.zeros((max(h1, h2), w1+w2, 3), dtype=np.uint8)

    # 将image1和透视变换后的image2放入拼接结果图像中
    result_image[:h1, :w1] = image1
    result_image[:h2, w1:] = result
    
    return result_image
```

#### 3.2 图像拼接2

1. 用SIFT提取图像中的特征点，并对每个关键点周围的区域计算特征向量。可以使用比SIFT快的SURF方法，但是我的opencv版本为最新版，不知道是专利的原因还是什么原因用SURF = cv2.xfeatures2D.SURF_create ()实例化的时候会报错，网上说可以退opencv版本，但是我这里没有尝试，就用了sift = cv2.SIFT_create()。
2. 分别提取好了两张图片的关键点和特征向量以后，可以利用它们进行两张图片的匹配。在拼接图片中，可以使用Knn进行匹配，但是使用FLANN快速匹配库更快，图片拼接，需要用到FLANN的单应性匹配。
3. 单应性匹配完之后可以获得透视变换H矩阵，用这个的逆矩阵来对第二幅图片进行透视变换，将其转到和第一张图一样的视角，为下一步拼接做准备。
4. 透视变化完后就可以直接拼接图片了，将图片通过numpy直接加到透视变化完成的图像的左边，覆盖掉重合的部分，得到拼接图片，但是这样拼接得图片中间会有一条很明显的缝隙，可以通过加权平均法，界线的两侧各取一定的比例来融合缝隙，速度快，但不自然。或者羽化法，或者拉普拉斯金字塔融合，效果最好。在这里用的是加权平均法，可以把第一张图叠在左边，但是对第一张图和它的重叠区做一些加权处理，重叠部分，离左边图近的，左边图的权重就高一些，离右边近的，右边旋转图的权重就高一些，然后两者相加，使得过渡是平滑地，这样看上去效果好一些，速度就比较慢。

<img src="picture\wps4.jpg" alt="img" style="zoom:80%;" />

在测试的时候发现直接拼接虽然可以拼接但是在拼接的地方会有一条很明显的缝隙。所以为了解决这个缝隙问题，引入了加权处理。

我们通常使用alpha因子，通常称为alpha通道，它在中心像素处的值为1，在与边界像素线性递减后变为0。当输出拼接图像中至少有两幅重叠图像时，我们将使用如下的alpha值来计算其中一个像素处的颜色：假设两个图像,在输出图像中重叠；每个像素点在图像，其中（R,G,B）是像素的颜色值，我们将在缝合后的输出图像中计算(x, y)的像素值。

<img src="picture\wps5.jpg" alt="img" style="zoom: 80%;" />

<img src="picture\wps6.jpg" alt="img" style="zoom:80%;" />

## 结果

### 1 改进 Harris 算法

下面是传统 Harris 算法提取出的角点与改进后的算法提取出的角点的对比图。（原图|传统 Harris |改进 Harris）

<img src="picture\image-20230630185316701.png" alt="image-20230630185316701" style="zoom:50%;" /><img src="picture\image-20230630185354176.png" alt="image-20230630185354176" style="zoom:50%;" /><img src="picture\image-20230630185425506.png" alt="image-20230630185425506" style="zoom:50%;" />

<img src="picture\image-20230630185929975.png" alt="image-20230630185929975" style="zoom: 40%;" /><img src="picture\image-20230630190050695.png" alt="image-20230630190050695" style="zoom:40%;" /><img src="picture\image-20230630190140083.png" alt="image-20230630190140083" style="zoom:40%;" />

下面是传统 Harris 算法提取出的角点与改进后的算法提取出的角点进行特征匹配的对比图。（原图|传统 Harris |改进 Harris）

<img src="picture\image-20230630185316701.png" alt="image-20230630185316701" style="zoom:50%;" /><img src="picture\image-20230630190856458.png" alt="image-20230630190856458" style="zoom:74%;" /><img src="picture\image-20230630190950190.png" alt="image-20230630190950190" style="zoom:74%;" />

<img src="picture\am.jpg" alt="am" style="zoom:50%;" /><img src="picture\image-20230630191709798.png" alt="image-20230630191709798" style="zoom:35%;" /><img src="picture\image-20230630191336579.png" alt="image-20230630191336579" style="zoom:35%;" />

### 2 改进 RANSAC 算法

特征匹配对比图。（未经 RANSAC 处理|经 RANSAC 处理）

<img src="picture\wps2.jpg" alt="img" style="zoom:45%;" /><img src="picture\wps3.jpg" alt="img" style="zoom:55%;" />

<img src="picture\image-20230630194102383.png" alt="image-20230630194102383" style="zoom: 67%;" /><img src="picture\image-20230630194152404.png" alt="image-20230630194152404" style="zoom:67%;" /><img src="picture\image-20230630190856458.png" alt="image-20230630190856458" style="zoom:67%;" /><img src="picture\image-20230630194441958.png" alt="image-20230630194441958" style="zoom:37%;" />

<img src="picture\image-20230630191336579.png" alt="image-20230630191336579" style="zoom:30%;" /><img src="picture\image-20230630194756844.png" alt="image-20230630194756844" style="zoom:30%;" />

可以发现匹配的乱七八糟的，而且有些地方还匹配错了。经过算法处理后，匹配的正确性大大提高。RANSAC算法核心原理其实就是去除外点，保留内点，因为内点匹配准确率更高，外点在这里会产生干扰。

### 3 图像拼接

#### 3.1 图像拼接1

通过**图像拼接1**拼接出的图像没能符合预期，以下是原图，拆分图以及拼接结果对比。

<img src="picture\am.jpg" alt="am" style="zoom:50%;" /><img src="picture\image-20230630201822241.png" alt="image-20230630201822241" style="zoom:60%;" /><img src="picture\image-20230630201842643.png" alt="image-20230630201842643" style="zoom:60%;" /><img src="picture\image-20230630201519442.png" alt="image-20230630201519442" style="zoom:39%;" />

#### 3.1 图像拼接2

通过**图像拼接2**拼接出的图像是正常的，以下是原图，拆分图以及拼接结果对比。

<img src="picture\wps8.jpg" alt="img" style="zoom:80%;" /><img src="picture\wps9.jpg" alt="img" style="zoom:60%;" /><img src="picture\wps10.jpg" alt="img" style="zoom:60%;" /><img src="picture\wps11.jpg" alt="img" style="zoom:80%;" />

## 总结和讨论

通过改进 Harris 角点检测算法和 RANSAC 算法，我们了解到了算法改进的重要性：在全景图拼接中，传统的方法存在着一些局限性，而通过改进算法，可以提高准确性和效率，解决一些问题。

在改进 Harris 角点检测算法时，我们对Harris 角点检测算法也有了更深入的了解。Harris 角点检测算法的基本思想是利用像素灰度值在不同方向上的变化来识别角点。角点是图像中像素灰度值变化较大的位置，而边缘和平坦区域的像素灰度值变化相对较小。Harris 角点检测算法具有一些优点，如对图像旋转、缩放、光照变化有较好的鲁棒性，并且计算效率较高。然而，它也存在一些限制，例如对于尺度变化和噪声敏感。文中对于Harris角点检测算法的改进具有以下优点：（1）提取角点质量更高：通过计算梯度乘积并进行高斯加权，改进的算法可以更准确地识别角点，将注意力集中在梯度幅度较大的像素点上。这样可以减少伪关键点的产生，提高角点的质量。（2）自适应阈值设置：传统的Harris算法使用固定阈值来筛选角点，可能导致忽略了一些有效的像素点。改进的算法采用自适应阈值设置方法，根据图像的特性和局部梯度分布动态地调整阈值，从而更精确地检测角点，并避免产生伪角点。这些改进使得角点检测算法在提取角点质量和速度方面都得到了优化。它们可以提高角点检测的准确性和鲁棒性，使得算法更适用于各种计算机视觉任务。

在改进 RANSAC 算法时，我们也认识到 RANSAC 算法核心原理其实就是去除外点，保留内点，因为内点匹配准确率更高，外点在这里会产生干扰。RANSAC 算法的优势在于它能够处理包含噪声和异常值的数据集。通过随机采样和内点筛选的过程，RANSAC 可以抵抗数据中的干扰，找到最佳的拟合模型。然而，该算法的性能受到阈值的选择以及迭代次数的影响，需要根据具体应用场景进行调整。文中提出根据不同特征点匹配度给予不同的权重，可以适当地提高 RANSAC 算法地性能。

在进行图像拼接1时，我的代码无法生成正确的拼接图，根据实验结果，猜想导致失败的原因应该是单应矩阵 H 的计算出现了错误。可以看到，image2 经过透视变换后，整体图像左移，移动方向是正确的，但是左移的幅度太大，导致跟 image1 无法配对。

在进行图像拼接2中，使用了 FLANN 的单应性匹配。对比 BFMatcher 匹配和 FLANN 匹配，BFMatcher 匹配的准确性更高，但是通过 RANSAC 算法可以消除两种匹配的差距。

基于以上思考，我们可以讨论一些今后可行的方案：

1. 深度学习方法：深度学习方法在全景图拼接中具有很大的潜力，可以带来显著的改进。以下是深度学习方法在全景图拼接中的几个关键方面：（1）特征提取与匹配：传统方法中，使用手工设计的特征点提取算法来获取图像特征点，然后进行匹配。而深度学习方法可以通过卷积神经网络（CNN）自动学习图像特征，无需依赖于人工设计的特征点算法。这可以提高特征提取的准确性和鲁棒性。（2）全景图生成：深度学习方法可以用于直接生成全景图像。通过训练一个生成模型，可以将多个图像进行输入，然后生成一张连续、无缝的全景图。这种端到端的生成方法可以避免中间环节的误差累积，得到更高质量的拼接结果。（3）变形和畸变校正：全景拼接过程中，图像可能存在透视投影和非刚性变形等畸变。深度学习方法可以借助于生成对抗网络（GAN）等技术，学习并校正这些畸变，从而获得更精确的拼接结果。
2. 多视角拼接：当前的全景图拼接主要是基于二维图像，可以探索多视角拼接的方法。通过使用多个相机或传感器，获取不同视角下的图像，并进行融合拼接。与传统的全景图拼接方法相比，多视角拼接可以提供更广阔的视角范围和更高的场景还原度。

## 个人贡献声明

江鹏程：Harris 角点检测算法改进，特征点匹配，图像拼接1，主要报告编写。

梅佳俊：RANSAC 算法实现，特征点匹配，图像拼接2。

## 引用参考

实验所用平台为 **vscode**。

参考文献：

[1]郑宇,张晓燕.基于改进特征点检测的全景图像拼接方法[J].信息记录材料,2022,23(12):162-164.

[2]洪改艳,芮廷先,俞伟广,等.Harris角点检测的优化算法 [J].计算机系统应用,2017,26(4):169-172.

[3] 赵明富,陈海军,宋涛,等.改进 RANSAC-SIFT 算法在图像匹配中的研究 [J].激光杂志,2018,39(1):114-118.